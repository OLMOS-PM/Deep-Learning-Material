{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 (Part I): Variational Inference with Neural Networks for a toy example\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "*Deep Learning. Master in Information Health Engineering *\n",
    "\n",
    "*Pablo M. Olmos pamartin@ing.uc3m.es*\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "Consider a certain number of sensors  placed at known locations, $\\mathbf{s}_1,\\mathbf{s}_2,\\ldots,\\mathbf{s}_L$. There is a target at an unknown position $\\mathbf{z}\\in\\mathbb{R}^2$ that is emitting a certain signal that is received at the $i$-th sensor with a signal strength distributed as follows:\n",
    "\n",
    "\\begin{align}\n",
    "x_i \\sim \\mathcal{N}\\Big(- A \\log\\left(||\\mathbf{s}_i-\\mathbf{z} ||^2\\right), \\sigma^2\\Big),\n",
    "\\end{align}\n",
    "\n",
    "where $A$ is a constant related to how fast signal strength degrades with distance. We assume a Gaussian prior for the unknown position $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0},\\mathbf{I})$. Given a set of $N$ i.i.d. samples for each sensor, $\\mathbf{X}\\in\\mathbb{R}^{L\\times N}$, we will use  Neural Variational Inference to find a Gaussian approximation to \n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{z}|\\mathbf{X}) \\propto  p(\\mathbf{X}|\\mathbf{z}) p(\\mathbf{z})\n",
    "\\end{align}\n",
    "\n",
    "Our approximation to $p(\\mathbf{z}|\\mathbf{X})$ is of the form\n",
    "\\begin{align}\n",
    "p(\\mathbf{z}|\\mathbf{X}) \\approx q(\\mathbf{z}|\\mathbf{X})=\\mathcal{N}\\Big(\\mu(\\mathbf{X}),\\Sigma(\\mathbf{X})\\Big),\n",
    "\\end{align}\n",
    "where\n",
    "\n",
    "- $\\mu(\\mathbf{X})$ --> Given by a Neural Network with parameter vector $\\theta$ and input $\\mathbf{X}$\n",
    "\n",
    "- $\\Sigma(\\mathbf{X})$ --> Diagonal covariance matrix, where the log of the main diagonal is constructed by a Neural Network with parameter vector $\\gamma$ and input $\\mathbf{X}$\n",
    "\n",
    "**Note:**  $\\mathbf{X}$ is reshaped into a $L\\times N$ vector to input both $\\mu(\\mathbf{X})$ and $\\Sigma(\\mathbf{X})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO lower-bound to $p(\\mathbf{X})$\n",
    "\n",
    "We will optimize $q(\\mathbf{z}|\\mathbf{X})$ w.r.t. $\\theta,\\gamma$ by optimizing the Evidence-Lower-Bound (ELBO):\n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{X}) &= \\int p(\\mathbf{X}|\\mathbf{z}) p(\\mathbf{z}) d\\mathbf{z}\\\\\n",
    "&\\geq \\int q(\\mathbf{X}|\\mathbf{z}) \\log \\left(\\frac{p(\\mathbf{X},\\mathbf{z})}{q(\\mathbf{X}|\\mathbf{z})}\\right)d\\mathbf{z}\\\\\n",
    "& = \\mathbb{E}_{q}\\left[\\log  p(\\mathbf{X}|\\mathbf{z})\\right] - D_{KL}(q(\\mathbf{z}|\\mathbf{X})||p(\\mathbf{z})\\triangleq \\mathcal{L}(\\mathbf{X},\\theta,\\gamma),\n",
    "\\end{align}\n",
    "where $D_{KL}(q(\\mathbf{z}|\\mathbf{X})||p(\\mathbf{z})$ is known in closed form since it is the KL divergence between two Gaussian pdfs:\n",
    "\n",
    "\\begin{align}\n",
    "D_{KL}(q(\\mathbf{z}|\\mathbf{X})||p(\\mathbf{z})) = \\frac{1}{2}Â \\left[\\text{tr}\\left(\\Sigma(\\mathbf{X})\\right)+\\left(\\mu(\\mathbf{X})^T\\mu(\\mathbf{X})\\right)-2-\\log\\det \\left(\\Sigma(\\mathbf{X})\\right) \\right]\n",
    "\\end{align}\n",
    "\n",
    "## SGD optimization\n",
    "\n",
    "- Sample $\\mathbf{\\epsilon}\\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})$\n",
    "- Sample from $q(\\mathbf{z}|\\mathbf{X})$:\n",
    "\\begin{align}\n",
    "\\mathbf{z}^0 = \\mu(\\mathbf{X}) + \\sqrt{\\text{diag}(\\Sigma(\\mathbf{X}))} \\circ \\mathbf{\\epsilon}\n",
    "\\end{align}\n",
    "- Compute gradients of \n",
    "\\begin{align}\n",
    "\\hat{\\mathcal{L}}(\\mathbf{X},\\theta,\\gamma) =\\log  p(\\mathbf{X}|\\mathbf{z}^0) - D_{KL}\\left(q(\\mathbf{z}|\\mathbf{X})||p(\\mathbf{z})\\right)\n",
    "\\end{align}\n",
    "\n",
    "w.r.t. $\\theta,\\gamma$. Note that, since the whole sample matrix is reshaped into a $L\\times N$ to input both $\\mu(\\mathbf{X})$ and $\\Sigma(\\mathbf{X})$, in this toy example **we do not use minibatches**! \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic model definition and generating samples\n",
    "\n",
    "We assume that all parameteres in the probabilistic model are known. To generate the sample matrix $\\mathbf{X}$, we fix the target position. Our goal is to run the inference method to estimate the target position exclusively from $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Elements of the true probabilistic model ####################\n",
    "\n",
    "loc_info = {}                           \n",
    "\n",
    "# Number o sensors\n",
    "loc_info['L'] = 3 \n",
    "\n",
    "# Number o sensors\n",
    "loc_info['z_dim'] = 2 \n",
    "\n",
    "#Position of sensors\n",
    "loc_info['pos_s'] = np.array([[0.5,1], [3.5,1], [2,3]]).astype(np.float32) \n",
    "\n",
    "# Target position (This is what we try to infer)\n",
    "loc_info['target'] = np.array([-1,2]).astype(np.float32)  \n",
    "\n",
    "#Variance of sensors (parameter sigma^2 in the probabilistic model)\n",
    "loc_info['var_s'] = 5.*np.ones(loc_info['L']).reshape([loc_info['L'],1]).astype(np.float32) \n",
    "\n",
    "#Attenuation factor per sensor (parameter A in the probabilistic model)\n",
    "loc_info['A'] = 10.0*np.ones(loc_info['L'],dtype=np.float32)\n",
    "\n",
    "# Number of measurements per sensor\n",
    "loc_info['N'] = 5 \n",
    "\n",
    "\n",
    "# Function to sample from the generative model\n",
    "def sample_X(L,M,z,pos_s,A,var_s):\n",
    "    \n",
    "    means = -1*A*np.log(np.sum((pos_s-z)**2,1))\n",
    "    \n",
    "    X = means.reshape([L,1]) + np.random.randn(L,M) * np.sqrt(var_s)\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling $\\mathbf{X}$ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample_X(loc_info['L'],loc_info['N'], loc_info['target'],loc_info['pos_s'],loc_info['A'],loc_info['var_s']).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the 2D scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loc_info['pos_s'][:,0],loc_info['pos_s'][:,1],'b>',label='Sensors',ms=15)\n",
    "plt.plot(loc_info['target'][0],loc_info['target'][1],'ro',label='Target',ms=15)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Defintion of the NNs that define the posterior mean and covariance matrix\n",
    "\n",
    "Given $\\mathbf{X}$ (reshaped into a $L\\times N$ vector), we will train two NNs to estimate both the mean and covariance matrix of the approximation to the posterior $q(\\mathbf{z}|\\mathbf{X})$. We assume a diagonal covariance matrix. In this way, the NN provides the diagonal of the covariance matrix.\n",
    "\n",
    "Each NN has one hidden layer with an hiperbolic tangent activation and no output activation. Since the diagonal of the covariance matrix can only contain positive numbers, we assume the network outpus the **logarithm of the diagonal of the covariance matrix variance**.\n",
    "\n",
    "> **Exercise:** Complete the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Posterior_moments(nn.Module):\n",
    "    def __init__(self,loc_info,hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        #z_dim=2 in our 2D-localization problem, but we leave\n",
    "        #as a free parameter\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.hidden_mean = nn.Linear(loc_info['L']*loc_info['N'],\n",
    "                                hidden_dim)\n",
    "        \n",
    "        self.hidden_var = nn.Linear(loc_info['L']*loc_info['N'],\n",
    "                                hidden_dim)        \n",
    "        \n",
    "        # Mean linear layer\n",
    "        self.output_mean = nn.Linear(hidden_dim, loc_info['z_dim'])\n",
    "        # Diagonal variance linear layer\n",
    "        self.output_logvar = nn.Linear(hidden_dim, loc_info['z_dim'])\n",
    "\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Complete the forward method.\n",
    "        \n",
    "        self.mean = # Your code here\n",
    "        \n",
    "        self.log_var = # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a class to evaluate the loss function (ELBO)\n",
    "\n",
    "Lets create a new class inherited from ``Posterior_moments`` that incorporates two new methods: one to sample from $q(\\mathbf{z}|\\mathbf{X})$ and one to evaluate the ELBO. Recall that to sample from from $q(\\mathbf{z}|\\mathbf{X})$, we first sample from $\\mathbf{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ and then\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{z}^0 = \\mu(\\mathbf{X}) + \\sqrt{\\text{diag}(\\Sigma(\\mathbf{X}))} \\circ \\mathbf{\\epsilon}\n",
    "\\end{align}\n",
    "\n",
    "> **Exercise:** Complete the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variational_Loss(Posterior_moments):\n",
    "    \n",
    "    def __init__(self,loc_info,hidden_dim):\n",
    "        super().__init__(loc_info,hidden_dim)\n",
    "        \n",
    "        self.loc_info = loc_info\n",
    "        \n",
    "        \n",
    "    def sample_from_q(self,x):\n",
    "        \n",
    "        self.forward(x)\n",
    "        \n",
    "        # Sampling from q(z|x). \n",
    "        # We first sample from N(0,I)\n",
    "        # Then, scale by std vector and sum the mean\n",
    "        \n",
    "        noise = torch.FloatTensor(x.shape[0], 1).normal_()\n",
    "        \n",
    "        self.sample = #Your code here\n",
    "        \n",
    "        \n",
    "    def ELBO(self,x):\n",
    "    \n",
    "        self.sample_from_q(x)\n",
    "        \n",
    "        # KL divergence ELBO regularizer\n",
    "        \n",
    "        self.KL = -0.5*self.loc_info['z_dim']+0.5*torch.sum(\n",
    "                torch.exp(self.log_var)+self.mean**2-self.log_var)\n",
    "        \n",
    "        # Evaluate log-likelihood for the current z sample\n",
    "        # To do this we use the probabilistic model!\n",
    "\n",
    "        # Signal mean per sensor\n",
    "        self.means = (-torch.from_numpy(self.loc_info['A'])*\n",
    "                                  torch.log(torch.sum((torch.from_numpy(self.loc_info['pos_s'])-self.sample)**2,1)))\n",
    "        \n",
    "        # Unnormalized likelihood\n",
    "        log_lik = -0.5 * (x.reshape([self.loc_info['L'],-1])-self.means.reshape([-1,1]))**2/torch.from_numpy(self.loc_info['var_s'])\n",
    "    \n",
    "        #Normalizing constant (not important for inference)\n",
    "        log_lik += -0.5*torch.log(torch.from_numpy(self.loc_info['var_s']))\n",
    "    \n",
    "        self.log_lik = torch.sum(log_lik)\n",
    "    \n",
    "        self.ELBO_loss = - #Your code Here. Multiply by -1 to minimize\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a class to perform ELBO optimization \n",
    "\n",
    "Create a new class inherited from `Variational_Loss` that implements a method to perform a SGD step over the variational bound (ELBO)\n",
    "\n",
    "> **Exercise:** Complete the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_variational_localization(Variational_Loss):\n",
    "    \n",
    "    def __init__(self,loc_info,hidden_dim):\n",
    "        \n",
    "        super().__init__(loc_info,hidden_dim)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=1e-2)\n",
    "        \n",
    "    # One SGD \n",
    "    def SGD_step(self,x):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        #Your code here\n",
    "        \n",
    "        #Your code here\n",
    "        \n",
    "        self.optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Training the inference model \n",
    "\n",
    "> **Exercise:** Complete the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 50\n",
    "\n",
    "m = neural_variational_localization(loc_info,hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_steps = 10000\n",
    "\n",
    "period_plot = 1000\n",
    "\n",
    "x = torch.from_numpy(X.reshape([1,-1]))\n",
    "\n",
    "for step in range(SGD_steps):\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    if(step % period_plot == 0):\n",
    "        \n",
    "        print(\"Step = %d, loglik = %.5f, KL = %.5f, ELBO = %.5f\" \n",
    "              %(step,m.log_lik,m.KL,m.ELBO_loss))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Compare posterior distribution with actual target position\n",
    "\n",
    "> **Exercise:** Complete the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior mean after training (use .detach().numpy() to convert to numpy)\n",
    "m_final = #Your code here\n",
    "\n",
    "# Posterior covariance matrix after training\n",
    "log_var_final = #Your code here\n",
    "\n",
    "\n",
    "nsamples = 50\n",
    "#Obtain nsamples from q(z|x)\n",
    "samples = #Your code here\n",
    "\n",
    "plt.plot(loc_info['pos_s'][:,0],loc_info['pos_s'][:,1],'b>',label='Sensors',ms=15)\n",
    "plt.plot(loc_info['target'][0],loc_info['target'][1],'ro',label='Target',ms=15)\n",
    "plt.plot(m_final[:,0],m_final[:,1],'g>',label='Post Mean')\n",
    "plt.scatter(samples[:,0],samples[:,1],label='Post Samples')\n",
    "plt.rcParams[\"figure.figsize\"] = [8,8]\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
